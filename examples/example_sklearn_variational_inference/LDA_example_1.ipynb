{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "###This example gives the implementation of LDA with variational inference using sklearn.LatentDirichletAllocation \n",
    "###There are three documents in this example in Chinese, the first and second are news about NBA, the third one is a new about\n",
    "###finance. \n",
    "import jieba\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "print(sys.getdefaultencoding())\n",
    "###import the modules needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "####read in first document nlp_nba_1.txt and split the words, then save it as nlp_nba_3.txt\n",
    "with open('./nlp_nba_1.txt') as f:\n",
    "    document = f.read()\n",
    "    \n",
    "    document_decode = document.encode('GBK')\n",
    "    document_cut = jieba.cut(document_decode)\n",
    "    result = ' '.join(document_cut)\n",
    "    with open('./nlp_nba_3.txt', 'w') as f2:\n",
    "        f2.write(result)\n",
    "f.close()\n",
    "f2.close() \n",
    "\n",
    "####read in first document nlp_nba_2.txt and split the words, then save it as nlp_nba_4.txt\n",
    "with open('./nlp_nba_2.txt') as f:\n",
    "    document2 = f.read()\n",
    "    \n",
    "    document2_decode = document2.encode('GBK')\n",
    "    document2_cut = jieba.cut(document2_decode)\n",
    "    #print  ' '.join(jieba_cut)\n",
    "    result = ' '.join(document2_cut)\n",
    "    with open('./nlp_nba_4.txt', 'w') as f2:\n",
    "        f2.write(result)\n",
    "f.close()\n",
    "f2.close() \n",
    "\n",
    "####read in first document nlp_finance_1.txt and split the words, then save it as nlp_finance_3.txt\n",
    "with open('./nlp_finance_1.txt') as f:\n",
    "    document3 = f.read()\n",
    "    \n",
    "    document3_decode = document3.encode('GBK')\n",
    "    document3_cut = jieba.cut(document3_decode)\n",
    "    #print  ' '.join(jieba_cut)\n",
    "    result = ' '.join(document3_cut)\n",
    "    with open('./nlp_finance_3.txt', 'w') as f3:\n",
    "        f3.write(result)\n",
    "f.close()\n",
    "f3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "首节 上来 ， 伦纳德 拿 手中 投入 网 ， 紧接着 ， 他 又 无视 防守 三分 命中 。 勇士 这边 ， 克莱 - 汤普森 状态 开 的 很早 ， 他 一人 接管 进攻 连得 9 分 。 但 此后 ， 猛龙 还是 把 控住 优势 的 一方 ， 进攻 端 ， 洛瑞 、 小 加索尔 、 西亚 卡姆 轮番 上分 ， 而 在 防守 端 ， 他们 延续 了 上场 球 的 强度 ， 死 掐 水花 兄弟 的 跑动 路线 。 勇士 运动战 受挫 ， 只能 靠着 造 犯规 罚球 入账 分数 。 反观 猛龙 ， 丹尼 - 格林 、 范 弗利 特 先后 杀到 内线 得分 ， 伊巴 卡补篮 也 中 ， 单节 打 完 ， 猛龙 27 - 26 暂时 领先 。 \n",
      " \n",
      " 次节 ， 猛龙 优势 愈发 明显 ， 本节 战至 7 分 12 秒 ， 小 加索尔 切入 上篮 得手 ， 洛瑞 三分 也 进 ， 分差 来到 两位数 。 接下来 ， 勇士 的 被动 还 在 持续 ， 尽管 ， 在 进攻 中 ， 水花 兄弟 能 持续 输出 。 但 在 防守 端 ， 勇士 却 极为 狼狈 ， 其中 一 球 ， 西亚 卡姆 吸引 包夹 妙 传小卡 ， 后者 面对 库里 盯防 ， 强行 上 反篮 打成 2 + 1 。 \n",
      " \n",
      " 勇士 防守 出 了 问题 ， 队中 球员 还 先后 遭遇 伤病 打击 ， 库里 、 伊戈 达拉 在 经过 短暂 治疗 后 返场 ， 卢尼 因为 肩伤 却 遗憾 退赛 了 。 可 就是 在 如此 被动 的 情况 下 ， 勇士 却 在 半场 结束 前 掀起 高潮 ， 库里 爆发 ， 他里 突外 投 独揽 10 分 ， 半场 打 完 ， 勇士 54 - 59 暂时 落后 。 \n",
      " \n",
      " 易边 再战 ， 猛龙 的 攻防 突然 诡异 断电 。 勇士 抓住机会 一通 狂攻 ， 三节 初段 竟然 打 了 对手 一波 18 - 0 。 范 弗利 特 三分 命中 ， 终于 帮 猛龙 止血 。 此后 ， 小卡 展现 巨星 风范 ， 开始 连续 得分 ， 帮主 队 追赶 。 比赛 重新 回归 拉锯 态势 ， 鲍威尔 翻身 跳投 得手 ， 小卡 上篮 也 中 ， 三节 打 完 ， 猛龙 80 - 88 落后 。 \n",
      " \n",
      " 末节 ， 勇士 再 遭 打击 ， 汤神 因为 左腿 扭伤 ， 之后 无奈 退赛 了 。 但 考辛 斯上篮 得手 ， 库克 命中 两粒 宝贵 三分球 ， 勇士 继续 握住 2 位数 的 优势 。 比赛 拼 到 最后 时段 ， 勇士 已然 弹尽粮绝 ， 进攻 端 取分 越来越 难 ， 但 猛龙 也 格外 急躁 ， 多次 强行 三分 出手 也 都 是 打铁 告终 ， 主控 洛瑞 还 吃 到 个人 第 6 次 犯规 被 罚 出场 。 尽管 ， 伦纳德 的 三罚 全中 和 丹尼 - 格林 的 三分 入网 ， 一度 又 让 猛龙 看到 追赶 希望 。 但 比赛 还 剩 7 秒 ， 伊戈 达拉 射中 关键 三分 。 全场 打 完 ， 勇士 109 - 104 险胜 猛龙 ， 总比分 被 扳成 1 - 1 平 。\n",
      "北京 时间 6 月 3 日 ， NBA 总决赛 第二场 ， 客场 作战 的 金州 勇士 以 109 - 104 击败 了 多伦多 猛龙 ， 将 总比分 扳为 1 - 1 平 。 格林 本场 17 分 10 篮板 9 助攻 的 准 三双 ， 可是 “ 追梦 ” 还 剩 51.4 秒 时 的 失误 险些 成为 勇士队 的 致命 毒药 。 \n",
      " \n",
      " 客场 作战 的 勇士队 开局 陷入 被动 ， 更 糟糕 的 是 ， 首发 中锋 考辛斯 4 分钟 内 两次 犯规 ， 而 汤普森 也 在 首节 只 打 了 7 分钟 就 身背 两次 犯规 被 换下 。 \n",
      " \n",
      " 此时 ， 落后 6 分 的 勇士队 急需解决 进攻 问题 。 当 西亚 卡姆 在 进攻 端 上演 空接 扣篮 后 ， 格林 连续 杀伤 猛龙队 内线 造成 杀伤 ， 他 带领 勇士队 打出 一波 10 - 0 的 攻击 波 ， “ 追梦 ” 贡献 了 其中 的 6 分 1 助攻 。 从 落后 到 领先 ， 唤醒 勇士 进攻 的 功臣 正是 格林 。 \n",
      " \n",
      " 第二节 格林 专职 防守 ， 将 进攻 完全 交给 了 “ 水花 兄弟 ” 。 从 最 多 落后 12 分到 半场 结束 只 相差 5 分 ， 格林 防守 端的 贡献 有目共睹 。 原本 ， 勇士队 可以 进一步 缩小 分差 ， 但是 格林 的 “ 不 冷静 ” 再次 惹祸 了 。 \n",
      " \n",
      " 还 剩 18 秒 时 ， 格林 防守 西亚 卡姆 。 此时 ， 西亚 卡姆 完全 没有 对 篮筐 的 威胁 ， 同时 也 没有 投篮 威胁 ， 格林 只 需要 保证 不失 位 就 可以 了 ， 但是 格林 却 不 冷静 的 犯规 了 。 此时 勇士 全队 犯规 数已 到 ， 西亚 卡姆 获得 了 罚球 得分 机会 。 对此 ， 库里 也 是 走上 来 劝 格林 不要 急躁 。 \n",
      " \n",
      " 勇士队 终于 在 第三节 打出 了 今年 总决赛 最 风调雨顺 的 一节 比赛 ， 球队 开局 一波 18 - 3 的 超级 攻击 波中 ， 格林 很 好 的 串联 了 球队 的 进攻 。 这 一节 中 ， 格林 6 分 2 篮板 3 助攻 ， 更令 “ 追梦 ” 兴奋 的 是 ， 一次 西亚 卡姆 内线 强打 ， 格林 劈头盖脸 的 送 上 了 一记 大帽 ， 这 也 让 格林 终于 可以 好好 发泄 之前 的 郁闷 之气 了 。 \n",
      " \n",
      " 今年 季后赛 ， 当 格林 取得 3 双 的 情况 下 ， 勇士队 只输 了 一场 。 第四节 比赛 ， 格林 先是 一次 空切 上篮 ， 之后 助攻 博格特 空中 接力 得分 ， 这是 格林 的 第 9 次 助攻 ， 距离 三双 数据 只差 一步 。 \n",
      " \n",
      " 距离 比赛 还 剩 51.4 秒 时 ， 格林 突破 内线 准备 将球 传给 考辛斯 ， 但是 这次 传球 失误 了 。 这次 致命 失误 导致 猛龙队 将 分差 缩小 为 两分 ， 幸好 伊戈 达拉 的 三分球 帮助 格林 “ 填 大坑 ” ， 这 也 让 勇士队 避免 了 被 逆转 ， 同时 也 让 格林 的 准 三双 数据 没有 白费 。\n",
      "券商 股早 盘显 强势 ， 华林 证券 大涨 7% 领涨 。 农业 股 持续 活跃 ， 敦煌 种业 、 登海 种业 涨停 ， 万向 德农 、 苏垦 农发 、 西部 牧业 、 新农 开发 、 北大荒 等 跟 涨 。 \n",
      " \n",
      " 　 　 环保 股 盘中 异动 ， 龙头 龙马 环卫 直线 封板 ， 绿色 动力 、 中国 天楹 一度 直线 涨停 ， 德创 环保 、 上海 环境 、 博天 环境 拉升 。 \n",
      " \n",
      " 　 　 A股 “ 穷 5 月 ” 已经 过去 ， 进入 6 月 ， 招商 证券 表示 ， A股 有望 迎来 绝境 逢生 ， 六月 中旬 是 一个 重要 分水岭 。 从 主观 判断 ， 出现 转机 的 概率 要 大于 继续 恶化 ， 届时 市场 可能 会 迎来 一波 值得 参与 的 反弹 。 操作 上 ， 投资者 应该 做好 两手 准备 ， 六月 中 上旬 在 市场 不 确定 较大 的 情况 下 ， 尽可能 以 防御 配置 为主 。 一旦 中 美 贸易 领域 出现 缓和 ， 市场 会 把握 有限 的 时间 窗口 ， 短期内 风险 偏好 提升 ， 券商 、 5G 、 电子 等 风险 偏好 改善 或者 受益 中 美 贸易 缓和 的 板块 可能 会 有 相对 收益 。 \n",
      " \n",
      " 　 　 首创 证券 认为 ， 近期 市场 继续 维持 弱势 ， 成交量 也 出现 了 显著 萎缩 。 虽然 市场 谨慎 预期 再起 ， 风险 偏好 遭到 压制 ， 以北 上 资金 为 代表 的 场内 资金 也 呈现 净 流出 。 但 越是 在 市场 出现 非理性 下跌 的 时候 ， 越是 长线 投资者 对 优质 价值 标的 进行 布局 的 时候 。 进一步 来看 ， 在 二季度 的 基本面 、 流动性 两大 预期 差 完全 落地 后 ， 叠加 外部 因素 扰动 之下 ， 市场 调整 压力 得到 了 充分 释放 ， 空间 上 的 调整 已经 基本 到位 ， 最 黑暗 的 时刻 其实 已经 过去 。 \n",
      " \n",
      " 　 　 安信 证券 则 称 ， 整体 来看 ， A股 市场 仍 处于 技术性 反弹 之中 ， 在 汇率 稳定 的 环境 下 ， 投资者 对于 流动性 的 乐观 预期 、 科创板 临近 预期 使得 市场 的 风险 偏好 边际 改善 ， 但 由于 当前 外部环境 依然 存在 较大 不确定性 ， 本次 反弹 空间 有限 。 ( 中 新 经纬 APP )\n"
     ]
    }
   ],
   "source": [
    "###read in corpus into memory\n",
    "with open('./nlp_nba_3.txt') as f3:\n",
    "    res1 = f3.read()\n",
    "print (res1)\n",
    "with open('./nlp_nba_4.txt') as f4:\n",
    "    res2 = f4.read()\n",
    "print (res2)\n",
    "with open('./nlp_finance_3.txt') as f5:\n",
    "    res3 = f5.read()\n",
    "print (res3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###import stop words list and transform it into dictionary\n",
    "stpwrdpath = \"stop_words.txt\"\n",
    "stpwrd_dic = open(stpwrdpath, 'rb')\n",
    "stpwrd_content = stpwrd_dic.read() \n",
    "stpwrdlst = stpwrd_content.splitlines()\n",
    "stpwrd_dic.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 270)\t1\n",
      "  (0, 252)\t1\n",
      "  (0, 486)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 103)\t1\n",
      "  (0, 106)\t1\n",
      "  (0, 207)\t1\n",
      "  (0, 221)\t1\n",
      "  (0, 374)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 100)\t1\n",
      "  (0, 102)\t1\n",
      "  (0, 33)\t1\n",
      "  (0, 118)\t1\n",
      "  (0, 53)\t1\n",
      "  (0, 61)\t1\n",
      "  (0, 173)\t1\n",
      "  (0, 265)\t1\n",
      "  (0, 119)\t1\n",
      "  (0, 186)\t1\n",
      "  (0, 249)\t1\n",
      "  (0, 328)\t1\n",
      "  (0, 437)\t1\n",
      "  (0, 158)\t1\n",
      "  :\t:\n",
      "  (2, 246)\t1\n",
      "  (2, 29)\t1\n",
      "  (2, 349)\t2\n",
      "  (2, 366)\t1\n",
      "  (2, 378)\t2\n",
      "  (2, 294)\t1\n",
      "  (2, 346)\t1\n",
      "  (2, 114)\t1\n",
      "  (2, 495)\t1\n",
      "  (2, 190)\t1\n",
      "  (2, 427)\t4\n",
      "  (2, 143)\t1\n",
      "  (2, 237)\t1\n",
      "  (2, 369)\t1\n",
      "  (2, 413)\t1\n",
      "  (2, 131)\t2\n",
      "  (2, 117)\t1\n",
      "  (2, 458)\t1\n",
      "  (2, 199)\t1\n",
      "  (2, 306)\t1\n",
      "  (2, 20)\t1\n",
      "  (2, 404)\t2\n",
      "  (2, 25)\t1\n",
      "  (2, 254)\t1\n",
      "  (2, 280)\t1\n"
     ]
    }
   ],
   "source": [
    "###process the corpus based on term frequency (TF)\n",
    "corpus = [res1,res2,res3]\n",
    "cntVector = CountVectorizer(stop_words=stpwrdlst)\n",
    "cntTf = cntVector.fit_transform(corpus)\n",
    "print (cntTf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00190568 0.99809432]\n",
      " [0.00191123 0.99808877]\n",
      " [0.9978381  0.0021619 ]]\n",
      "[[0.50013201 0.50013146 0.50013146 ... 1.49994745 1.49994745 1.49994745]\n",
      " [3.49986799 2.49986854 2.49986854 ... 0.50005255 0.50005255 0.50005255]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/software/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "###build LDA models\n",
    "lda = LatentDirichletAllocation(n_topics=2,\n",
    "                                learning_offset=50.,\n",
    "                                random_state=2)\n",
    "docres = lda.fit_transform(cntTf)\n",
    "\n",
    "\n",
    "### print topic distribution and word distribution\n",
    "print (docres) ###We can see that the first and second docs belong to topic 2, and the third doc belongs to topic 1.\n",
    "print (lda.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
